{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70ed1a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from note_seq.midi_io import midi_file_to_note_sequence\n",
    "import torch\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00789f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba68d9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logmel(audio_numpy, sr) :\n",
    "    S = librosa.feature.melspectrogram(y=audio_numpy, sr=sr, n_mels=128, fmax=12000)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "    return S_dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59ef1adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_OF(num) :\n",
    "    note_instruments = {36 : 1, 39 : 2, 42 : 3, 49 : 4}\n",
    "\n",
    "    # input preprocess\n",
    "    y, sr = librosa.load(f'dataset/full/beatbox/{num}.wav')\n",
    "    logmel = torch.Tensor(get_logmel(y, sr))\n",
    "\n",
    "    midi = midi_file_to_note_sequence(f'dataset/full/midi/{num}.mid')\n",
    "    v = logmel.shape[1]/midi.total_time\n",
    "    onset = torch.zeros(logmel.shape[1])\n",
    "    frameset = torch.zeros(logmel.shape[1])\n",
    "    \n",
    "#     print(onset.shape, offset.shape)\n",
    "\n",
    "    for note in midi.notes :\n",
    "        onset[int(note.start_time*v)] = 1\n",
    "        for n in range(int(note.start_time*v), int(note.end_time*v)-1) :\n",
    "            frameset[n] = note_instruments[note.pitch]\n",
    "    return logmel, onset, frameset, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5bf6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_OF(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10293a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "note_instruments = {36 : 1, 39 : 2, 42 : 3, 49 : 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4e83a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2327263c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 200 300 400 500 600 700 800 900 1000 \n",
      "1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 \n",
      "2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 \n",
      "3100 3200 3300 3400 3500 3600 3700 3800 3900 4000 \n",
      "4100 4200 4300 Done\n"
     ]
    }
   ],
   "source": [
    "input_train = []\n",
    "output_onset_train = []\n",
    "output_frameset_train = []\n",
    "vs = []\n",
    "\n",
    "for i in range(1, 4376) :\n",
    "    input_, output_onset_, output_frameset_, v = preprocess_OF(i)\n",
    "    \n",
    "    if input_.shape[1] <= 300 :\n",
    "        to_app_inp = torch.zeros((input_.shape[0], PATCH))\n",
    "        to_app_inp[:,:input_.shape[1]] = input_\n",
    "        \n",
    "        to_app_out_on = torch.zeros(PATCH)\n",
    "        to_app_out_on[:output_onset_.shape[0]] = output_onset_\n",
    "        \n",
    "        to_app_out_off = torch.zeros(PATCH)\n",
    "        to_app_out_off[:output_frameset_.shape[0]] = output_frameset_\n",
    "        \n",
    "        input_train.append(to_app_inp.numpy())\n",
    "        output_onset_train.append(to_app_out_on.numpy())\n",
    "        output_frameset_train.append(to_app_out_off.numpy())\n",
    "        vs.append(v)\n",
    "        \n",
    "    elif input_.shape[1] > 300 and input_.shape[1] <= 600 :\n",
    "        \n",
    "        to_app_inp1 = torch.zeros((input_.shape[0], PATCH))\n",
    "        to_app_inp2 = torch.zeros((input_.shape[0], PATCH))\n",
    "        \n",
    "        to_app_out1_on = torch.zeros(PATCH)\n",
    "        to_app_out2_on = torch.zeros(PATCH)\n",
    "        to_app_out1_off = torch.zeros(PATCH)\n",
    "        to_app_out2_off = torch.zeros(PATCH)\n",
    "        \n",
    "        to_app_inp1 = input_[:,:PATCH]\n",
    "        to_app_inp2 = input_[:,-PATCH:]\n",
    "        to_app_out1_on = output_onset_[:PATCH]\n",
    "        to_app_out2_on = output_onset_[-PATCH:]\n",
    "        to_app_out1_off = output_frameset_[:PATCH]\n",
    "        to_app_out2_off = output_frameset_[-PATCH:]\n",
    "        \n",
    "        input_train.append(to_app_inp1.numpy())\n",
    "        input_train.append(to_app_inp2.numpy())\n",
    "        output_onset_train.append(to_app_out1_on.numpy())\n",
    "        output_onset_train.append(to_app_out2_on.numpy())\n",
    "        \n",
    "        output_frameset_train.append(to_app_out1_off.numpy())\n",
    "        output_frameset_train.append(to_app_out2_off.numpy())\n",
    "        \n",
    "        vs.append(v)\n",
    "        vs.append(v)\n",
    "    \n",
    "    else :\n",
    "        print('Error')\n",
    "        \n",
    "    if i%100 == 0: \n",
    "        print(i, end=' ')\n",
    "    if i%1000 == 0:\n",
    "        print()\n",
    "        \n",
    "dill.dump(torch.Tensor(input_train), open('logmels_patch300_train', 'wb'))\n",
    "dill.dump(torch.Tensor(output_onset_train), open('onset_patch300_train', 'wb'))\n",
    "dill.dump(torch.Tensor(output_frameset_train), open('frameset_patch300_train', 'wb'))\n",
    "dill.dump(torch.Tensor(vs), open('v_patch300_train', 'wb'))\n",
    "print('Done')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c7d6aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4400 4500 4600 4700 4800 4900 5000 \n",
      "5100 5200 5300 5400 5500 5600 5700 5800 5900 6000 \n",
      "6100 6200 Done\n"
     ]
    }
   ],
   "source": [
    "input_train = []\n",
    "output_onset_train = []\n",
    "output_frameset_train = []\n",
    "vs = []\n",
    "\n",
    "for i in range(4376, 6251) :\n",
    "    input_, output_onset_, output_frameset_, v = preprocess_OF(i)\n",
    "    \n",
    "    if input_.shape[1] <= 300 :\n",
    "        to_app_inp = torch.zeros((input_.shape[0], PATCH))\n",
    "        to_app_inp[:,:input_.shape[1]] = input_\n",
    "        \n",
    "        to_app_out_on = torch.zeros(PATCH)\n",
    "        to_app_out_on[:output_onset_.shape[0]] = output_onset_\n",
    "        \n",
    "        to_app_out_off = torch.zeros(PATCH)\n",
    "        to_app_out_off[:output_frameset_.shape[0]] = output_frameset_\n",
    "        \n",
    "        input_train.append(to_app_inp.numpy())\n",
    "        output_onset_train.append(to_app_out_on.numpy())\n",
    "        output_frameset_train.append(to_app_out_off.numpy())\n",
    "        vs.append(v)\n",
    "        \n",
    "    elif input_.shape[1] > 300 and input_.shape[1] <= 600 :\n",
    "        \n",
    "        to_app_inp1 = torch.zeros((input_.shape[0], PATCH))\n",
    "        to_app_inp2 = torch.zeros((input_.shape[0], PATCH))\n",
    "        \n",
    "        to_app_out1_on = torch.zeros(PATCH)\n",
    "        to_app_out2_on = torch.zeros(PATCH)\n",
    "        to_app_out1_off = torch.zeros(PATCH)\n",
    "        to_app_out2_off = torch.zeros(PATCH)\n",
    "        \n",
    "        to_app_inp1 = input_[:,:PATCH]\n",
    "        to_app_inp2 = input_[:,-PATCH:]\n",
    "        to_app_out1_on = output_onset_[:PATCH]\n",
    "        to_app_out2_on = output_onset_[-PATCH:]\n",
    "        to_app_out1_off = output_frameset_[:PATCH]\n",
    "        to_app_out2_off = output_frameset_[-PATCH:]\n",
    "        \n",
    "        input_train.append(to_app_inp1.numpy())\n",
    "        input_train.append(to_app_inp2.numpy())\n",
    "        output_onset_train.append(to_app_out1_on.numpy())\n",
    "        output_onset_train.append(to_app_out2_on.numpy())\n",
    "        \n",
    "        output_frameset_train.append(to_app_out1_off.numpy())\n",
    "        output_frameset_train.append(to_app_out2_off.numpy())\n",
    "        \n",
    "        vs.append(v)\n",
    "        vs.append(v)\n",
    "    \n",
    "    else :\n",
    "        print('Error')\n",
    "        \n",
    "    if i%100 == 0: \n",
    "        print(i, end=' ')\n",
    "    if i%1000 == 0:\n",
    "        print()\n",
    "        \n",
    "dill.dump(torch.Tensor(input_train), open('logmels_patch300_test', 'wb'))\n",
    "dill.dump(torch.Tensor(output_onset_train), open('onset_patch300_test', 'wb'))\n",
    "dill.dump(torch.Tensor(output_frameset_train), open('frameset_patch300_test', 'wb'))\n",
    "dill.dump(torch.Tensor(vs), open('v_test', 'wb'))\n",
    "print('Done')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576e8e72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
